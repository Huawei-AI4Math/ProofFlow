# ProofFlow

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

ProofFlow is a  Python package that automatically converts natural language mathematical proofs into formalized Lean 4 code using Large Language Models (LLMs). This package is designed to bridge the gap between informal mathematical reasoning and formal verification systems, making mathematical proofs machine-verifiable and accessible to automated theorem provers.

## ğŸš€ Features

- **Intelligent Proof Graph Generation**: Automatically decomposes natural language proofs into structured dependency graphs
- **Multi-Model Support**: Compatible with various LLMs including Claude, GPT, Gemini, and custom vLLM servers
- **Lean 4 Integration**: Generates valid Lean 4 code with automatic verification
- **Interactive Visualizations**: Create both static and interactive proof dependency graphs
- **Error Analysis**: Comprehensive error detection and analysis for debugging formalizations
- **Semantic Scoring**: Evaluate the quality of formalized proofs using AI-powered scoring
- **Flexible Architecture**: Support for both DAG-based and sequential proof processing

## ğŸ“¦ Installation

### Prerequisites

- Python 3.8 or higher
- Lean 4 (for local verification) or access to a Lean server

### Install from Source

```bash
git clone https://github.com/your-username/proofflow.git
cd proofflow
pip install -e .
```

## ğŸ—ï¸ Project Structure

```
proofflow/
â”œâ”€â”€ proofflow/                  # Main package directory
â”‚   â”œâ”€â”€ __init__.py            # Package initialization
â”‚   â”œâ”€â”€ proofflow.py            # Core ProofFlow class
â”‚   â”œâ”€â”€ proof_graph.py         # Proof graph generation and validation
â”‚   â”œâ”€â”€ proof_formalize.py     # Natural language to Lean formalization
â”‚   â”œâ”€â”€ proof_prover.py        # Automated proof generation
â”‚   â”œâ”€â”€ proof_scorer.py        # Semantic scoring of formalized proofs
â”‚   â”œâ”€â”€ lean_check.py          # Lean 4 verification utilities
â”‚   â”œâ”€â”€ utils.py               # Utility functions and LLM management
â”‚   â”œâ”€â”€ io.py                  # Input/output operations
â”‚   â””â”€â”€ vis.py                 # Visualization utilities
â”œâ”€â”€ prompts/                   # LLM prompt templates
â”‚   â”œâ”€â”€ proof_graph.md         # Proof graph generation prompts
â”‚   â”œâ”€â”€ lemma_formalizer.md    # Formalization prompts
â”‚   â”œâ”€â”€ lemma_prover.md        # Proof generation prompts
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/                      # Sample datasets
â”‚   â””â”€â”€ benchmark_0409.json    # Benchmark dataset
â”œâ”€â”€ benchmark_results/         # Benchmark results and outputs
â”œâ”€â”€ example.ipynb             # Comprehensive usage examples
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ setup.py                  # Package configuration
â””â”€â”€ README.md                 # This file
```

## ğŸš€ Quick Start

### Basic Usage

```python
from proofflow import ProofFlow, LLMManager, LeanServer

# Set up Lean server (local or remote)
lean_server = LeanServer(api_url="http://localhost:14457")  # Remote server
# OR
# lean_server = LeanServer(project_path="/path/to/mathlib")  # Local project

# Configure LLM models
graph_model = LLMManager(
    model_info={
        "api_key": "your-api-key",
        "base_url": "your-base-url",
        "model": "your-model-name",
    },
    system_prompt_path="prompts/proof_graph.md",
)

formalize_model = LLMManager(
    model_info={
        "api_key": "your-api-key",
        "base_url": "your-base-url",
        "model": "your-model-name",
    },
    system_prompt_path="prompts/lemma_formalizer.md",
)

solver_model = LLMManager(
    model_info={
        "api_key": "your-api-key",
        "base_url": "your-base-url",
        "model": "your-model-name",
    },
    system_prompt_path="prompts/lemma_prover.md",
)

# Initialize ProofFlow
proof_flow = ProofFlow(
    lean_server=lean_server,
    graph_model_manager=graph_model,
    formalize_model_manager=formalize_model,
    solver_model_manager=solver_model,
    verbose=True
)

# Process a natural language proof
nl_proof = """
Theorem: For all real numbers x, y, if xÂ² + yÂ² = 1, then |x| â‰¤ 1.
Proof: Since xÂ² â‰¥ 0 and yÂ² â‰¥ 0, we have xÂ² + yÂ² â‰¥ xÂ². 
Given that xÂ² + yÂ² = 1, we get 1 â‰¥ xÂ², which means xÂ² â‰¤ 1. 
Taking the square root of both sides, we obtain |x| â‰¤ 1.
"""

# Run formalization
proof_flow.autoformalize_series(nl_proof)

# Get results
lean_code = proof_flow.get_lean_code()
print(lean_code)

# Generate visualizations
proof_flow.plot_dag("proof_dag.png")
proof_flow.interactive_dag("proof_dag.html")

# Get performance summary
summary = proof_flow.summary()
print(f"Formalization accuracy: {summary['form_acc']:.2%}")
print(f"Proof success rate: {summary['solv_acc']:.2%}")
```

## ğŸ”§ Configuration

### LLM Models

AutoFormalize supports multiple LLM providers:

- **OpenRouter**: Claude, GPT, Gemini models
- **OpenAI**: GPT-3.5, GPT-4, GPT-4o
- **Custom vLLM servers**: Local model hosting
- **Hugging Face**: Transformers models

### Lean 4 Setup

Choose between local or server-based Lean verification:

```python
# Local Lean project (requires mathlib)
lean_server = LeanServer(project_path="/path/to/your/lean/project")

# Remote Lean server
lean_server = LeanServer(api_url="http://your-lean-server:port")
```

## ğŸ“Š Advanced Features

### Semantic Scoring

```python
# Add scoring model
score_model = LLMManager(
    model_info={
        "api_key": "your-api-key",
        "base_url": "your-base-url",
        "model": "your-model-name",
    },
    system_prompt_path=None,
)

proof_flow = ProofFlow(
    # ... other parameters
    score_model_manager=score_model
)

# Compute proof scores after autoformalize series
proof_flow.proof_score(aggregation="katz", verbose=True)
print(f"Total proof score: {proof_flow.total_score}")
```

### Error Analysis

```python
# Perform comprehensive error analysis
proof_flow.error_analysis(
    score_threshold=0.6,
    prover_retries=3,
    verbose=True
)

# Access error reports for each proof step
for item in proof_flow.proof_items:
    if hasattr(item, 'error_report'):
        print(f"Step {item.id}: {item.error_report['error_type']}")
```


### Visualization

```python
# Create static proof graph
proof_flow.plot_dag("proof_structure.png")

# Create interactive HTML visualization
proof_flow.interactive_dag("interactive_proof.html")
```

## ğŸ§ª Benchmarking and Reproducibility

To benchmark the Goedel Formalizer and Solver, run the benchmark.sh script. This script automates the entire process, but it's very time-consuming and can take several days to complete.

Before you start, you'll need to fill out the .env file with the necessary API keys and URLs for your services. This includes the OpenAI API, as well as the Goedel Formalizer and Solver (including their model locations), and the Lean server. An example .env file is left on the main folder, with the API key fields left blank for you to fill in.

Due to potential connection timeouts and rate limits with some services, it's a good idea to run the commands in benchmark.sh one by one in your terminal. If these issues occurs, just retry it to repeat the missing problems. Once the script finishes, the resultsâ€”including autoformalization files (.pickle, .html) and summary tables (.xlsx)â€”will be stored in the benchmark_results/ folder.

## ğŸ“ˆ Performance Metrics

AutoFormalize tracks detailed performance metrics:

- **Formalization Accuracy**: Percentage of successfully formalized proof steps
- **Proof Success Rate**: Percentage of steps that can be automatically proven
- **ProofScore**: A novel composite score for the whole proof formalization, taking into account semanting similarity between natural language and Lean code
- **Token Usage**: Total tokens consumed across all LLM calls
- **Processing Time**: Time breakdown by model and operation


## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

**ProofFlow** - Bridging the gap between informal mathematics and formal verification.